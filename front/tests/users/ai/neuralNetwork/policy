policy

value function

model

{
	S -> a finite set of states
	A -> a finite set of actions
	P -> a state transition probability matrix
	R -> reward function
	y -> discount factor
}


value function 

s0 -> s1 -> final

value(s1) = s0 + s1 + final

value of state = from state, average the value of each possible action from that state, which gives the value of the current state

value of action = from action, average the value of each possible state reached by that action.

conbine both for value of state (basically look ahead 2, like minmax but averaging all possible moves moving forward)